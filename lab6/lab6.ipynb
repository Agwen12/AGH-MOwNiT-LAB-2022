{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg as sln\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from num2words import num2words\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "nltk.download('omw-1.4')\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Przygotuj duży (>1000 elementów) zbiór dokumentów tekstowych w języku an-gielskim (np. wybrany korpus tekstów, podzbiór artykułów Wikipedii, zbiór doku-mentówHTMLuzyskanych za pomocąWeb crawlera, zbiór rozdziałów wyciętych zróżnych książek)\n",
    "\n",
    "Wybrany zbiór składa się z dumpa wikipedii simple. Cały set składa się z 205328 stron, jednak ze względu na ograniczoną moc obliczeniową został ograniczony do 20000 w podstawowej wersji i 2000 dla wersji z usuwaniem szumu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (C:\\Users\\User\\.cache\\huggingface\\datasets\\wikipedia\\20220301.simple\\2.0.0\\aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cec3b05451c4ed6b6c26c7d470710fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
    "dataset = dataset_dict['train']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['id', 'url', 'title', 'text'],\n    num_rows: 205328\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "DOCUMENT_COUNT = dataset.num_rows"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Funkcja używana do preprocessingu tekstów. Usuwanie \"stop words\", zamienie liter na małe, itd."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "lemma = nltk.WordNetLemmatizer()\n",
    "def preprocess(dicti):\n",
    "    text = dicti['text']\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"_\",\"\")\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub('  +',' ',text)\n",
    "\n",
    "    text_list = text.split()\n",
    "    text = \"\"\n",
    "\n",
    "    for word in text_list:\n",
    "        word = lemma.lemmatize(word, pos=\"v\")\n",
    "        try: word = num2words(int(word))\n",
    "        except ValueError: pass\n",
    "        if word in stop_words: continue\n",
    "\n",
    "        text += word + \" \"\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    dicti['processed'] = text\n",
    "    return dicti\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/205328 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db44b76218a54c458dfb4c6b580974d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda e: preprocess(e))\n",
    "with open(\"dataset\", \"wb\") as file:\n",
    "    pickle.dump(dataset, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "with open(\"dataset\", \"rb\") as d:\n",
    "    dataset = pickle.load(d)\n",
    "\n",
    "DOCUMENT_COUNT = 2_000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': Value(dtype='string', id=None),\n 'url': Value(dtype='string', id=None),\n 'title': Value(dtype='string', id=None),\n 'text': Value(dtype='string', id=None),\n 'processed': Value(dtype='string', id=None)}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Określ  słownik  słów  kluczowych  (termów)  potrzebny  do  wyznaczenia  wektorówcechbag-of-words(indeksacja). Przykładowo zbiorem takim może być unia wszyst-kich słów występujących we wszystkich tekstach.\n",
    "\n",
    "Do tokenizacji został Tokenizer z biblioteki Tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 11266.09it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts([dataset[i]['processed'] for i in tqdm(range(DOCUMENT_COUNT))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "with open(\"tokenizer_2K\", \"wb\") as file:\n",
    "    pickle.dump(tokenizer, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "54672"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "with open(\"tokenizer_2K\", \"rb\") as d:\n",
    "    tokenizer = pickle.load(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Dla każdego dokumentujwyznacz wektor cechbag-of-words $d_j$ zawierający częstości występowania poszczególnych słów (termów) w tekście.\n",
    " Zbuduj rzadką macierz wektorów cechterm-by-document matrixw której wekto-ry cech ułożone  są kolumnowo $A_{m × n}= [d_1|d_2|...|d_n]$ (m jest liczbą  termów  wsłowniku, anliczbą dokumentów)\n",
    " w moim przypadku wektory układane są rzędami, a nie kolumnami. Jest to brane pod uwagę w kolejnych krokach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [00:02<00:00, 136.40it/s]\n"
     ]
    }
   ],
   "source": [
    "BATCH = 400\n",
    "TBD_matrix = sparse.csr_matrix(tokenizer.texts_to_matrix([dataset[i]['processed'] for i in range(DOCUMENT_COUNT//BATCH)], mode=\"count\"), dtype=np.float64)\n",
    "\n",
    "\n",
    "for i in tqdm(range(1, BATCH)):\n",
    "    matrix = sparse.csr_matrix(tokenizer.texts_to_matrix([dataset[i]['processed']\n",
    "              for i in range((DOCUMENT_COUNT//BATCH)*i, (DOCUMENT_COUNT//BATCH)*(i+1) )], mode=\"count\"), dtype=np.float64)\n",
    "    TBD_matrix = sparse.vstack([TBD_matrix, matrix])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "with open(\"vTBD_matrix_2K\", \"wb\") as file:\n",
    "    pickle.dump(TBD_matrix, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Przetwórz wstępnie otrzymany zbiór danych mnożąc elementybag-of-wordsprzezinverse document frequency. Operacja ta pozwoli na redukcję znaczenia często wy-stępujących słów. $IDF(w) = \\frac {logN} {n_w}$, gdzie $ n_w $ jest liczbą dokumentów, w których występuje słowow, aNjest całkowitąliczbą dokumentów."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "IDF = np.zeros(shape=(TBD_matrix.shape[1]))\n",
    "for idx, w in tokenizer.index_word.items():\n",
    "    IDF[idx] = np.log(DOCUMENT_COUNT / tokenizer.word_docs[w])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "TBD_matrix = TBD_matrix.multiply(IDF).tocsr()\n",
    "with open(\"vTBD_matrix_IDF_2K\", \"wb\") as d:\n",
    "    pickle.dump(TBD_matrix, d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "with open(\"vTBD_matrix_IDF_2K\", \"rb\") as d:\n",
    "    TBD_matrix = pickle.load(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Napisz  program  pozwalający  na  wprowadzenie  zapytania  (w  postaci  sekwencjisłów)  przekształcanego  następnie  do  reprezentacji  wektorowejq(bag-of-words).Program ma zwrócić k dokumentów najbardziej zbliżonych do podanego zapytania. Użyj korelacji między wektorami jako miary podobieństwa\n",
    "$$ cosθ_j = \\frac {q^Td_j} {‖q‖‖d_j‖} = \\frac {q^TAe_j}{‖q‖‖Ae_j‖} $$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def similarity_metric_1(q, matrix, i):\n",
    "    return (matrix.getrow(i).dot(q[0]) / sln.norm(matrix.getrow(i)))[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def search(query, matrix, metric , k=10):\n",
    "    cos = []\n",
    "    q = tokenizer.texts_to_matrix([query], mode=\"binary\")\n",
    "    q_norm = np.linalg.norm(q[0])\n",
    "    q = q / q_norm\n",
    "\n",
    "    for i in tqdm(range(DOCUMENT_COUNT)):\n",
    "        cos.append(metric(q, matrix, i))\n",
    "\n",
    "    # top_k = np.argpartition(cos, -k)[-k:]\n",
    "    top_k = np.argsort(cos)[-k:][::-1]\n",
    "    for i in top_k:\n",
    "        print(\"Title: {:<35} close: {:<20} url: {:<34}\".format(dataset[int(i)]['title'], cos[int(i)], dataset[int(i)]['url']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 4284.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Roman                               close: 0.5808212199281281   url: https://simple.wikipedia.org/wiki/Roman\n",
      "Title: Roman Empire                        close: 0.2954126184236678   url: https://simple.wikipedia.org/wiki/Roman%20Empire\n",
      "Title: Holy Roman Empire                   close: 0.2523634709164451   url: https://simple.wikipedia.org/wiki/Holy%20Roman%20Empire\n",
      "Title: 9                                   close: 0.24050016434573168  url: https://simple.wikipedia.org/wiki/9\n",
      "Title: 8                                   close: 0.20692046254854818  url: https://simple.wikipedia.org/wiki/8\n",
      "Title: Saint David                         close: 0.19168945637541088  url: https://simple.wikipedia.org/wiki/Saint%20David\n",
      "Title: 7                                   close: 0.18462009873706187  url: https://simple.wikipedia.org/wiki/7\n",
      "Title: Venus (mythology)                   close: 0.17636578571505107  url: https://simple.wikipedia.org/wiki/Venus%20%28mythology%29\n",
      "Title: 12                                  close: 0.1725848000984011   url: https://simple.wikipedia.org/wiki/12\n",
      "Title: Yatton                              close: 0.14211765834809892  url: https://simple.wikipedia.org/wiki/Yatton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search(\"roman\", TBD_matrix, similarity_metric_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Zastosuj normalizację wektorów cechdji wektoraq, tak aby miały one długość 1. Użyj zmodyfikowanej miary podobieństwa otrzymując $|q^TA|= [|cosθ_1|,|cosθ_2|,...,|cosθ_n|] $"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<2000x54673 sparse matrix of type '<class 'numpy.float64'>'\n\twith 393827 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "normalize(TBD_matrix, norm='l1', axis=1, copy=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9999999999999971"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TBD_matrix.getrow(0).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "with open(\"vTBD_normalized\", \"rb\") as d:\n",
    "    TBD_matrix = pickle.load(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "with open(\"vTBD_normalized_2K\", \"wb\") as d:\n",
    "    pickle.dump(TBD_matrix, d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "W  celu  usunięcia  szumu  z  macierzyAzastosuj  SVD  i $low$ $rank$ $approximation $ otrzymując\n",
    "$$ A \\approx A_k = U_kD_kV^T_k= [u_1 | \\hdots | u_k]\\begin{bmatrix} σ_1 & & \\\\ & \\ddots & \\\\ & &σ_k\n",
    " \\end{bmatrix} \\begin{bmatrix}v^T_1\\\\ \\vdots \\\\ v^T_k \\end{bmatrix}=\\sum_{i=1}^{k} σ_iu_iv_i^T $$\n",
    "oraz nową miarę podobieństwa\n",
    "$$cosθ_j = \\frac {q^TA_ke_j}{‖q‖‖A_ke_j‖}$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import scipy.sparse.linalg as ssl\n",
    "u, s, vt = ssl.svds(TBD_matrix, k=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "u, s, vt  = sparse.csr_matrix(u, dtype=np.float32), sparse.diags(s) , sparse.csr_matrix(vt, dtype=np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "with open(\"tuple3_2K\", \"wb\") as d:\n",
    "    pickle.dump((u, s, vt), d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "low_rank = u @ s @ vt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"low_rank\", \"wb\") as d:\n",
    "    pickle.dump(low_rank, d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def similarity_metric_2(q, matrix, i):\n",
    "    return (q[0].transpose() @ matrix).getrow(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"low_rank\", \"rb\") as d:\n",
    "    low_rank = pickle.load(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search(\"tank\", low_rank, similarity_metric_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"vTBD_matrix_2K\", \"rb\") as d:\n",
    "    m = pickle.load(d)\n",
    "search(\"april month\", m, similarity_metric_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Po usunięciu szumu z macierzy (k = 1000) wyniki wydają się nieco lepsze. W wynikach bez aproksymacji w wynikach pojawiają się mało sensowne pozycje (1992, 2005, 1603). W wersji z usuwaniem dalej się one pojawiają, ale są gorzej oceniane."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search(\"april month\", TBD_matrix, similarity_metric_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bez wykorzystania IDF wyniki są znacznie słabsze jakościowo. IDF obniża wartość popularnych słów, co sprawia, że słowa kluczowe mają większy wpływ na ostateczny wynik wyszukiwania."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}